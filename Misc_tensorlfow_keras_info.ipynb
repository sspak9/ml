{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gotchas when using Keras/Tensorflow\n",
    "\n",
    "couple things to note when using tensorflow as your backend for keras\n",
    "\n",
    "### ALWAYS import keras as from tensorflow import keras\n",
    "If you just import keras without using the from tensorflow, things may work a bit, but there are places where it WILL break\n",
    "\n",
    "Before noticing some weird behavior, I always used:\n",
    "```python\n",
    "import numpy as np\n",
    "import keras\n",
    "```\n",
    "\n",
    "The \"issue\" did not pop up until I was using `fit_generator()` when something that worked great basically threw weird error\n",
    "(Note. I only found out about the issue while playing around using mxnet as keras backend. MXnet states to use `channels_first`.\n",
    "\n",
    "### channel_first vs channel_last\n",
    "When using cuDNN library from NVidia as part of CUDA package, it's best to set the keras.json content to use channels_first\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"floatx\": \"float32\",\n",
    "    \"epsilon\": 1e-07,\n",
    "    \"backend\": \"tensorflow\",\n",
    "    \"image_data_format\": \"channels_first\"\n",
    "}\n",
    "```\n",
    "\n",
    "Let's check out the benchmark using the typical tensorflow default of `\"channels_last\"`. This is the content of the keras.json file:\n",
    "```json\n",
    "{\n",
    "    \"floatx\": \"float32\",\n",
    "    \"epsilon\": 1e-07,\n",
    "    \"backend\": \"tensorflow\",\n",
    "    \"image_data_format\": \"channels_last\"\n",
    "}\n",
    "```\n",
    "\n",
    "I am not going to run the code but the model used as a benchmark using MNIST data is listed below:\n",
    "\n",
    "```text\n",
    "# define the model: \n",
    "model = keras.models.Sequential()\n",
    "model.add( keras.layers.Conv2D(32, kernel_size=(3,3), input_shape=input_shape , activation='relu' ))\n",
    "model.add( keras.layers.Dropout(rate=0.05))\n",
    "model.add( keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu' ))\n",
    "model.add( keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add( keras.layers.Dropout(rate=0.5))\n",
    "\n",
    "model.add( keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu' ))\n",
    "model.add( keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu' ))\n",
    "model.add( keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add( keras.layers.Dropout(rate=0.5))\n",
    "\n",
    "model.add( keras.layers.Flatten())\n",
    "model.add( keras.layers.Dense(265, activation='relu'))\n",
    "model.add( keras.layers.Dropout(rate=0.5))\n",
    "model.add( keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# compile to model\n",
    "model.compile(optimizer='adam',\n",
    "\t\t\tloss='categorical_crossentropy',\n",
    "\t\t\tmetrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "```\n",
    "### The output\n",
    "I am using old NVidia GTX 770 (with 2GB VRam) - a slow card - to run the benchmark\n",
    "```\n",
    "\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #\n",
    "=================================================================\n",
    "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320\n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 26, 26, 32)        0\n",
    "_________________________________________________________________\n",
    "conv2d_2 (Conv2D)            (None, 24, 24, 32)        9248\n",
    "_________________________________________________________________\n",
    "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0\n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 12, 12, 32)        0\n",
    "_________________________________________________________________\n",
    "conv2d_3 (Conv2D)            (None, 10, 10, 64)        18496\n",
    "_________________________________________________________________\n",
    "conv2d_4 (Conv2D)            (None, 8, 8, 64)          36928\n",
    "_________________________________________________________________\n",
    "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0\n",
    "_________________________________________________________________\n",
    "dropout_3 (Dropout)          (None, 4, 4, 64)          0\n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)          (None, 1024)              0\n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 265)               271625\n",
    "_________________________________________________________________\n",
    "dropout_4 (Dropout)          (None, 265)               0\n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 10)                2660\n",
    "=================================================================\n",
    "Total params: 339,277\n",
    "Trainable params: 339,277\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "Train on 60000 samples, validate on 10000 samples\n",
    "Epoch 1/5\n",
    "60000/60000 [==============================] - 11s 191us/step - loss: 0.4193 - acc: 0.8634 - val_loss: 0.0685 - val_acc: 0.9775\n",
    "Epoch 2/5\n",
    "60000/60000 [==============================] - 9s 151us/step - loss: 0.1197 - acc: 0.9632 - val_loss: 0.0414 - val_acc: 0.9861\n",
    "Epoch 3/5\n",
    "60000/60000 [==============================] - 9s 151us/step - loss: 0.0880 - acc: 0.9732 - val_loss: 0.0313 - val_acc: 0.9903\n",
    "Epoch 4/5\n",
    "60000/60000 [==============================] - 9s 151us/step - loss: 0.0719 - acc: 0.9778 - val_loss: 0.0299 - val_acc: 0.9906\n",
    "Epoch 5/5\n",
    "60000/60000 [==============================] - 9s 151us/step - loss: 0.0622 - acc: 0.9809 - val_loss: 0.0258 - val_acc: 0.9920\n",
    "```\n",
    "\n",
    "### The same code but with image_data_format set to \"channels_first\"\n",
    "```\n",
    "Train on 60000 samples, validate on 10000 samples\n",
    "Epoch 1/5\n",
    "2019-03-07 04:56:08.997189: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library cublas64_100.dll locally\n",
    "60000/60000 [==============================] - 10s 169us/step - loss: 0.3917 - acc: 0.8727 - val_loss: 0.0565 - val_acc: 0.9812\n",
    "Epoch 2/5\n",
    "60000/60000 [==============================] - 9s 144us/step - loss: 0.1133 - acc: 0.9652 - val_loss: 0.0395 - val_acc: 0.9881\n",
    "Epoch 3/5\n",
    "60000/60000 [==============================] - 9s 145us/step - loss: 0.0827 - acc: 0.9750 - val_loss: 0.0286 - val_acc: 0.9912\n",
    "Epoch 4/5\n",
    "60000/60000 [==============================] - 9s 144us/step - loss: 0.0698 - acc: 0.9782 - val_loss: 0.0245 - val_acc: 0.9919\n",
    "Epoch 5/5\n",
    "60000/60000 [==============================] - 9s 143us/step - loss: 0.0591 - acc: 0.9823 - val_loss: 0.0216 - val_acc: 0.9928\n",
    "```\n",
    "\n",
    "### As you can see, the channels_first is about 4% faster. On a faster GPU and running a more complex model, the speed up will be 2x faster ( according to MXNet folks )\n",
    "\n",
    "### The issue with whether you import keras or from tensorflow import keras\n",
    "First the keras version is a bit different depending on how you import\n",
    "\n",
    "```python\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "```\n",
    "This prints: `2.2.4.1`\n",
    "\n",
    "```python\n",
    "from tensorflow import keras\n",
    "print(keras.__version__)\n",
    "```\n",
    "This prints: `2.2.4-tf`\n",
    "\n",
    "### The Issue\n",
    "The issue with just importing keras came up when running `fit_generator()`:\n",
    "```python\n",
    "# use datagen against it\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "  rotation_range = 45,\n",
    "  width_shift_range=0.15,\n",
    "  height_shift_range=0.15,\n",
    "  zoom_range = 0.2,\n",
    "  fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# this will generate few parameters to be used by data gen\n",
    "datagen.fit(x_train)\n",
    "\n",
    "fit_history2 = model.fit_generator(\n",
    "  datagen.flow(x_train,y_train,batch_size=200),\n",
    "  epochs = 150,\n",
    "  validation_data = (x_test, y_test)\n",
    ")\n",
    "```\n",
    "\n",
    "The error message thrown were:\n",
    "```text\n",
    "ValueError: `steps_per_epoch=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `steps_per_epoch` or use the `keras.utils.Sequence` class.\n",
    "```\n",
    "\n",
    "Of course, I could have fixed the issue by adding `steps_per_epoch`. Somehow the tensorflow version \"knew\" what that value was but default keras did not.\n",
    "\n",
    "```python\n",
    "fit_history2 = model.fit_generator(\n",
    "  datagen.flow(x_train,y_train,batch_size=200),\n",
    "  steps_per_epoch=len(x_train),\n",
    "  epochs = 150,\n",
    "  validation_data = (x_test, y_test)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
